{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-alarm",
   "metadata": {},
   "source": [
    "## Importing Spacy\n",
    "\n",
    "SpaCy is a library that allows you to perform many NLP tasks, including preprocessing text by exploiting not only a rule based approach, but also some pretrained models, for example to split a text into sentences, or find the named entities contained in the text.\n",
    "\n",
    "To take advantage of it, you need to download it as explained in the following cell. For english, a common choice is `en_core_web_sm`, but also `en_core_web_lg` results in a good performance while being lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-pressing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:42.520625Z",
     "start_time": "2021-03-28T20:32:42.518920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "## I recommend the one above, because the following is more accurate but less efficient\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frank-monaco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.391180Z",
     "start_time": "2021-09-08T08:26:58.137087Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# You can also load en_core_web_lg that has an higher accuracy but it's less efficient\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee85a78",
   "metadata": {},
   "source": [
    "By running `nlp.pipeline` you can see what are the steps that spaCy automatically does for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brutal-usage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.400494Z",
     "start_time": "2021-09-08T08:26:59.398316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000002034992FFA0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000002034992F160>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x00000203499B0040>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x00000203499C38C0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x00000203499B6C40>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x00000203499B0190>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45ebf8",
   "metadata": {},
   "source": [
    "To use it, if you have loaded the model in a variable named `nlp` as above (`nlp = spacy.load(\"en_core_web_sm\")`), you can just pass the text you want as argument as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "marine-rescue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.446450Z",
     "start_time": "2021-09-08T08:26:59.406845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a935a5",
   "metadata": {},
   "source": [
    "By doing so, a lot of things happened! You can iterate over the \"doc\" and you will get the tokens of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behavioral-prospect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.897610Z",
     "start_time": "2021-09-08T08:26:59.890403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Antonio\n",
      "is\n",
      "learning\n",
      "Python\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-shooting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:02.666773Z",
     "start_time": "2021-09-08T08:27:02.662516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d5c82",
   "metadata": {},
   "source": [
    "The spaCy model automatically divide the text in sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2252cca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:04.580743Z",
     "start_time": "2021-09-08T08:27:04.571436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    }
   ],
   "source": [
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9eef52",
   "metadata": {},
   "source": [
    "It could look a trivial task, like it was a fancy way of doing `text.split(\".\")`. However, how would you split into sentences the text \"Im antonio im learning python\"?\n",
    "\n",
    "We know that \"im\" is a wrong way to write \"I'm\" and, since there are two verbs in that text, there are also two sentences. Let's see how spaCy performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b16de73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:40.775962Z",
     "start_time": "2021-09-08T08:27:40.753581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im antonio im learning python\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Im antonio im learning python\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3ada5",
   "metadata": {},
   "source": [
    "Nice! However, don't get used to this, because there are tons of more complicated sentences that can easily be misinterpreted.\n",
    "\n",
    "In the example below, you can see that spaCy is smart enough to consider N.Y. as a single token, and not as two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phantom-intranet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:47:57.191508Z",
     "start_time": "2021-09-08T08:47:57.058897Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = nlp(\"Let's go to N.Y.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "designed-capitol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:47:57.982864Z",
     "start_time": "2021-09-08T08:47:57.977939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-duplicate",
   "metadata": {},
   "source": [
    "As you have seen, using `nlp`, that comes from `spacy.load(\"en_core_web_sm\")`, you get the tokenized version of the sentence. If you want only the instance of the `Tokenizer` class, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accomplished-marker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.054850Z",
     "start_time": "2021-03-28T20:32:44.050350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "spacy.tokenizer.Tokenizer"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-medium",
   "metadata": {},
   "source": [
    "If you want to instantiate a custom one, with rules and prefixes and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "olive-mother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.062007Z",
     "start_time": "2021-03-28T20:32:44.055855Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(vocab=nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-clerk",
   "metadata": {},
   "source": [
    "The tokenizer defined above contains only english rules.\n",
    "Let's test it on \"Let's go to N.Y.!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "distinct-trade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.068307Z",
     "start_time": "2021-03-28T20:32:44.063416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.!\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-county",
   "metadata": {},
   "source": [
    "As you can see here, it doesn't handle the exceptions about the dots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-consumer",
   "metadata": {},
   "source": [
    "Looking at the output of `nlp.pipeline` above, we can see there are a tagger, a dependency parser and the entity recognizer. Let's check the entities of the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sufficient-radar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:51:09.371036Z",
     "start_time": "2021-09-08T08:51:09.332008Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is a $1000b company.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mighty-twenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:51:09.736977Z",
     "start_time": "2021-09-08T08:51:09.731898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "a\n",
      "$\n",
      "1000b\n",
      "company\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "convenient-census",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.172302Z",
     "start_time": "2021-03-28T20:32:44.167426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "1000b DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-cassette",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-brief",
   "metadata": {},
   "source": [
    "In general, it's convenient to remove all the stop words, *i.e. very common words in a language*, because they don't help most of NLP problem such as semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "going-palace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.290058Z",
     "start_time": "2021-03-28T20:32:44.283484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['say', 'this', 'no', 'herein', 'enough', 'there', 'and', 'thereafter', 'really', 'although']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(\"Number of stop words: %d\" % len(spacy_stopwords))\n",
    "print(\"First ten stop words: %s\" % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-tokyo",
   "metadata": {},
   "source": [
    "To remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "crucial-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.317300Z",
     "start_time": "2021-03-28T20:32:44.291085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determined\n",
      "drop\n",
      "litigation\n",
      "monastry\n",
      ",\n",
      "relinguish\n",
      "claims\n",
      "wood\n",
      "-\n",
      "cuting\n",
      "\n",
      "\n",
      "fishery\n",
      "rihgts\n",
      ".\n",
      "ready\n",
      "becuase\n",
      "rights\n",
      "valuable\n",
      ",\n",
      "\n",
      "\n",
      "vaguest\n",
      "idea\n",
      "wood\n",
      "river\n",
      "question\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-death",
   "metadata": {},
   "source": [
    "For adding customized stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acknowledged-bahamas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:30.546200Z",
     "start_time": "2021-03-28T20:33:30.528780Z"
    }
   },
   "outputs": [],
   "source": [
    "customize_stop_words = [\"computing\", \"filtered\"]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-scene",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word ‘play’ can be used as ‘playing’, ‘played’, ‘plays’, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Let’s first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    " \n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "worthy-toilet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:36.947990Z",
     "start_time": "2021-03-28T20:33:36.465998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['determine',\n 'drop',\n 'litigation',\n 'monastry',\n ',',\n 'relinguish',\n 'claim',\n 'wood',\n '-',\n 'cut',\n '\\n',\n 'fishery',\n 'rihgts',\n '.',\n 'ready',\n 'becuase',\n 'right',\n 'valuable',\n ',',\n '\\n',\n 'vague',\n 'idea',\n 'wood',\n 'river',\n 'question',\n '.']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "# not using merge_chunk_nouns\n",
    "doc = nlp(\n",
    "    u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    ")\n",
    "\n",
    "lemma_word1 = []\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    lemma_word1.append(token.lemma_)\n",
    "lemma_word1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-large",
   "metadata": {},
   "source": [
    "## Removing the punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tested-lafayette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.236544Z",
     "start_time": "2021-03-28T20:33:38.220156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'He determined to drop his litigation with the monastry and relinguish his claims to the woodcuting and \\nfishery rihgts at once He was the more ready to do this becuase the rights had become much less valuable and he had \\nindeed the vaguest idea where the wood and river in question were'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-commerce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.840465Z",
     "start_time": "2021-03-28T20:33:38.808474Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text_no_punct)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-source",
   "metadata": {},
   "source": [
    "For text extracted from dialogues or chats, it is convenient to preprocess the text so that multiple occurrences of the same characters get condensed into one or two, and then use a spell checker to find the correct form of the word.\n",
    "\n",
    "A way to do that is to replace all the occurrences of repeated characters with a single one and then use a spell checker: \"hhheeelllllooo hoooowww areee youuu?\" becomes \"helo how are you?\" and then the spell checker would make it \"hello how are you?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "greater-cabin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:46.506377Z",
     "start_time": "2021-03-28T20:44:46.492492Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2312/3947625657.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mst\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"hhheeeLLLLooo hoooowww areee youuu?????\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mre\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msub\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr\"(.)\\1+\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mr\"\\1\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mst\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "st = \"hhheeeLLLLooo hoooowww areee youuu?????\"\n",
    "text = re.sub(r\"(.)\\1+\", r\"\\1\", st)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-rapid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:48.622525Z",
     "start_time": "2021-03-28T20:44:48.518501Z"
    }
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "text = nlp(text)\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown([token.text for token in text])\n",
    "\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-marriage",
   "metadata": {},
   "source": [
    "It didn't find any mispelled (even if there was \"helo\"). Try another spell checker:\n",
    "\n",
    "https://github.com/fsondej/autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-occupation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:43:37.953727Z",
     "start_time": "2021-03-28T20:43:37.889573Z"
    }
   },
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "spell(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-language",
   "metadata": {},
   "source": [
    "As you can see, it's not always working properly! However, overall it should improve your text.\n",
    "\n",
    "If you want to create a separate lemmatizer instead of having it in the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-dressing",
   "metadata": {},
   "source": [
    "**For spacy before v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-portrait",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:49:41.875502Z",
     "start_time": "2021-03-30T19:49:41.866457Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "print(lemmatizer(\"studying\", VERB))\n",
    "print(lemmatizer(\"studying\", NOUN))\n",
    "print(lemmatizer(\"studying\", ADJ))\n",
    "\n",
    "# or as alternative\n",
    "\n",
    "print(lemmatizer.verb(\"studying\"))\n",
    "print(lemmatizer.noun(\"studying\"))\n",
    "print(lemmatizer.adj(\"studying\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-header",
   "metadata": {},
   "source": [
    "spaCy has no built-in stemming! However, Lemmatization is enough for most of the tasks. As alternative, you can use [NLTK library](https://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-showcase",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-steering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:36:29.855279Z",
     "start_time": "2021-03-31T04:36:29.828402Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-chair",
   "metadata": {},
   "source": [
    "The `.pos_` attribute gives the *coarse-grained* POS tag. To inspect the *fine-grained* POS tags we could use the `.tag_`attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-beads",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:38:16.398804Z",
     "start_time": "2021-03-31T04:38:16.359022Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-arrow",
   "metadata": {},
   "source": [
    "While the output of the `.pos_` attribute is easy to decrypt (`PROPN`: proper noun,\n",
    "`AUX`: Auxiliary verb,\n",
    "`VERB`: verb,\n",
    "`ADP`: Adposition,\n",
    "`PUNCT`: Punctuation), the `.tag_`'s output is more cryptic. For this, you can use the `spacy.explain()` function to get the intuition behind that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-cement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:42:21.613532Z",
     "start_time": "2021-03-31T04:42:21.602635Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in sentence:\n",
    "    print(spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-mercy",
   "metadata": {},
   "source": [
    "Go and dig up your primary school grammar book!\n",
    "\n",
    "Let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-bracelet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:45:40.533471Z",
     "start_time": "2021-03-31T04:45:40.515639Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in sentence:\n",
    "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-canberra",
   "metadata": {},
   "source": [
    "(the numbers between curly brackets define spaces for a better formatting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-greek",
   "metadata": {},
   "source": [
    "You can count the number of occurrences of each POS tag by calling the `count_by` method. \n",
    "\n",
    "The syntax is as follows (you need to pass `spacy.attrs.POS` as argument of the method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-cambodia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:01:14.881989Z",
     "start_time": "2021-03-31T05:01:14.735095Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(\"Antonio is learning Python Programming Language\")\n",
    "\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-selection",
   "metadata": {},
   "source": [
    "The keys of the vocabulary are the ID of the POS tags, the values are their frequencies of occurrence. To retrieve the POS tags given the ID, you can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-bermuda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:55.476564Z",
     "start_time": "2021-03-31T05:05:55.466354Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence.vocab[96].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-military",
   "metadata": {},
   "source": [
    "where 96 is the ID of the tag. Printing all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-thursday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:04.754457Z",
     "start_time": "2021-03-31T05:05:04.749748Z"
    }
   },
   "outputs": [],
   "source": [
    "for ID, frequency in num_pos.items():\n",
    "    print(f\"{ID} stands for {sentence.vocab[ID].text:{8}}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-chart",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-generation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:34.612659Z",
     "start_time": "2021-03-29T06:04:34.586397Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Antonio works at Strive School.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-swimming",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:49.092105Z",
     "start_time": "2021-03-29T06:04:49.075027Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-bookmark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:02.488436Z",
     "start_time": "2021-03-29T06:06:02.459044Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Rome is a big city.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-reservoir",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:05.912053Z",
     "start_time": "2021-03-29T06:06:05.901532Z"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-gospel",
   "metadata": {},
   "source": [
    "ORG stands for organization, GPE stands for Geopolitical Entity. Some other tags are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-soldier",
   "metadata": {},
   "source": [
    "In spaCy you can list the entities by doing:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-preserve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.282842Z",
     "start_time": "2021-03-31T05:54:59.233747Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Manchester United is looking to sign Harry Kane for $90 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-waste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.773187Z",
     "start_time": "2021-03-31T05:54:59.766740Z"
    }
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-hospital",
   "metadata": {},
   "source": [
    "We can access the entities text, label by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-lincoln",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:56:13.150302Z",
     "start_time": "2021-03-31T05:56:13.141509Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-giant",
   "metadata": {},
   "source": [
    "Even if the entities are self-explanatory for this example, you can use `spacy.explain()` for a detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-nicaragua",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:57:32.686265Z",
     "start_time": "2021-03-31T05:57:32.678082Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-springer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:11:28.227061Z",
     "start_time": "2021-03-31T06:11:28.210463Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sentence, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-trance",
   "metadata": {},
   "source": [
    "We can also filter which entity to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-scenario",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:13.012412Z",
     "start_time": "2021-03-31T06:12:12.922949Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(u'Manchester United is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars')\n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-buffer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:25.772261Z",
     "start_time": "2021-03-31T06:12:25.766031Z"
    }
   },
   "outputs": [],
   "source": [
    "filter = {'ents': ['ORG']}\n",
    "displacy.render(sentence, style='ent', jupyter=True, options=filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f01af1",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To deal with text, it's often convenient to wrap all the preprocessing you want to do in a single function. Let's define one that:\n",
    "\n",
    "- split into tokens\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- make everything lowercase\n",
    "- lemmatize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0cd15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:31:18.079420Z",
     "start_time": "2021-09-08T13:31:18.054166Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    params sentence: a str containing the sentence we want to preprocess\n",
    "    return the tokens list\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49f43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:31:30.004823Z",
     "start_time": "2021-09-08T13:31:29.882100Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing(\"This is a sentence I'm going to preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4025b",
   "metadata": {},
   "source": [
    "As you can see, removing \"stopwords\" can be brutal, cause you will be missing a lot of important words that helps for the meaning of the sentence. For example, we missed the word \"I\". For this reason, sometimes is better to specify a list of words that you think won't be meaningful for the task you're going to perform. A good way to find them, is by checking the most frequent words in the corpus you have.\n",
    "\n",
    "Let's say you have this cell as text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f2d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:35:11.460396Z",
     "start_time": "2021-09-08T13:35:11.456638Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"As you can see, removing \"stopwords\" can be brutal, cause you will be missing a lot of important words that helps for the meaning of the sentence. For example, we missed the word \"I\". For this reason, sometimes is better to specify a list of words that you think won't be meaningful for the task you're going to perform. A good way to find them, is by checking the most frequent words in the corpus you have.\n",
    "\n",
    "Let's say you have this cell as text:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428b49f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735ad4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:36:08.224301Z",
     "start_time": "2021-09-08T13:36:08.220013Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74b2f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:36:42.531023Z",
     "start_time": "2021-09-08T13:36:42.517764Z"
    }
   },
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a05eb",
   "metadata": {},
   "source": [
    "By typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586b516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:38:44.806425Z",
     "start_time": "2021-09-08T13:38:44.799951Z"
    }
   },
   "outputs": [],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40830c",
   "metadata": {},
   "source": [
    "we get the 10 most common words. Let's say that I'm trying to get the gist of the text, then I could say that I get rid of \"the\" and \"a\" words. \n",
    "\n",
    "In that case, my preprocessing function becomes something like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59334f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:40:21.545018Z",
     "start_time": "2021-09-08T13:40:21.531682Z"
    }
   },
   "outputs": [],
   "source": [
    "STOPWORDS = [\"the\", \"a\"]\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    params sentence: a str containing the sentence we want to preprocess\n",
    "    return the tokens list\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.lemma_ in STOPWORDS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c1e8a",
   "metadata": {},
   "source": [
    "where you see I replaced the control `not token.is_stop()` with `not token.lemma_ in STOPWORDS`.\n",
    "\n",
    "As usual, be aware of the task you are going to perform is very important to improve the chances of an high accuracy instead of blindly applying a set of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4f7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}